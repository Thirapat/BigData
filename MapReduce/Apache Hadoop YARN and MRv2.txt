########## Hand on Apache Hadoop YARN and MRv2 ##########

Step 1 : ## Create java file ##
cd
nano WordCount.java
## Step 1.1 : Copy to all command to WordCount.java ##
########## begin WordCount.java  ##########
import java.io.IOException;
import java.util.StringTokenizer;
import org.apache.hadoop.conf.Configuration;
import org.apache.hadoop.fs.Path;
import org.apache.hadoop.io.IntWritable;
import org.apache.hadoop.io.Text;
import org.apache.hadoop.mapreduce.Job;
import org.apache.hadoop.mapreduce.Mapper;
import org.apache.hadoop.mapreduce.Reducer;
import org.apache.hadoop.mapreduce.lib.input.FileInputFormat;
import org.apache.hadoop.mapreduce.lib.output.FileOutputFormat;

public class WordCount {
	public static class TokenizerMapper extends Mapper<Object, Text, Text, IntWritable>{
		private final static IntWritable one = new IntWritable(1);
		private Text word = new Text();
		public void map(Object key, Text value, Context context) throws IOException, InterruptedException {
			StringTokenizer itr = new StringTokenizer(value.toString());
			while (itr.hasMoreTokens()) {
				word.set(itr.nextToken());
				context.write(word, one);
			}
		}
	}

	public static class IntSumReducer extends Reducer<Text,IntWritable,Text,IntWritable> {
		private IntWritable result = new IntWritable();
		public void reduce(Text key, Iterable<IntWritable> values,Context context) throws IOException, InterruptedException {
			int sum = 0;
			for (IntWritable val : values) {
				sum += val.get();
			}
			result.set(sum);
			context.write(key, result);
		}
	}

	public static void main(String[] args) throws Exception {
		Configuration conf = new Configuration();
		Job job = Job.getInstance(conf, "word count");
		job.setJarByClass(WordCount.class);
		job.setMapperClass(TokenizerMapper.class);
		job.setCombinerClass(IntSumReducer.class);
		job.setReducerClass(IntSumReducer.class);
		job.setOutputKeyClass(Text.class);
		job.setOutputValueClass(IntWritable.class);
		FileInputFormat.addInputPath(job, new Path(args[0]));
		FileOutputFormat.setOutputPath(job, new Path(args[1]));
		System.exit(job.waitForCompletion(true) ? 0 : 1);
	}
}
########## end WordCount.java  ##########

Step 2 : Compile .java to .class 

Step 2.1 : Create directory  wordcount_classes 
mkdir wordcount_classes

Step 2.2 : Run command javac 
javac -classpath /usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar -d wordcount_classes/ WordCount.java

Step 2.3 :
jar -cvf ./wordcount.jar -C wordcount_classes/ . ##<== end of line##

Step 3: Execute yarn 
yarn jar ./wordcount.jar WordCount /inputs/* /outputs/wordcount_output_dir001

Step 4 : Open another terminal to check Yarn Processes are running 
jps 

Step 5: Review the results from Map Reduce Command ##

hdfs dfs -ls /outputs
hdfs dfs -ls /outputs/wordcount_output_dir001
hdfs dfs -cat /outputs/wordcount_output_dir001/part-r-00000

Step 6 : ## Review Linux File System ##
??
cd /var/hadoop_data/namenode/current/
ls -l
cd /var/hadoop_data/datanode/current/
ls -l

